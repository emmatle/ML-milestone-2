Running 504 experiments (216 MLP + 288 CNN)...


=== Experiment 1/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.8411
Train set: accuracy = 83.697% - F1-score = 0.650752
Validation set:  accuracy = 74.691% - F1-score = 0.470722


=== Experiment 2/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.2435
Train set: accuracy = 84.755% - F1-score = 0.690256
Validation set:  accuracy = 75.167% - F1-score = 0.494849


=== Experiment 3/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 3.3823
Train set: accuracy = 88.180% - F1-score = 0.780917
Validation set:  accuracy = 72.217% - F1-score = 0.445846


=== Experiment 4/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 2.7116
Train set: accuracy = 87.189% - F1-score = 0.745404
Validation set:  accuracy = 71.646% - F1-score = 0.419923


=== Experiment 5/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.0775
Train set: accuracy = 88.348% - F1-score = 0.780186
Validation set:  accuracy = 72.788% - F1-score = 0.425509


=== Experiment 6/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.6483
Train set: accuracy = 87.189% - F1-score = 0.772975
Validation set:  accuracy = 72.978% - F1-score = 0.469600


=== Experiment 7/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.0981
Train set: accuracy = 86.216% - F1-score = 0.743297
Validation set:  accuracy = 74.025% - F1-score = 0.433082


=== Experiment 8/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.3666
Train set: accuracy = 86.921% - F1-score = 0.748793
Validation set:  accuracy = 72.693% - F1-score = 0.443956


=== Experiment 9/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.8365
Train set: accuracy = 88.298% - F1-score = 0.774505
Validation set:  accuracy = 73.168% - F1-score = 0.428146


=== Experiment 10/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.7851
Train set: accuracy = 87.038% - F1-score = 0.755058
Validation set:  accuracy = 73.168% - F1-score = 0.415659


=== Experiment 11/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.7146
Train set: accuracy = 86.971% - F1-score = 0.729511
Validation set:  accuracy = 71.931% - F1-score = 0.406840


=== Experiment 12/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.5154
Train set: accuracy = 86.081% - F1-score = 0.724221
Validation set:  accuracy = 73.359% - F1-score = 0.418294


=== Experiment 13/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.6674
Train set: accuracy = 82.975% - F1-score = 0.622040
Validation set:  accuracy = 72.598% - F1-score = 0.395585


=== Experiment 14/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.5217
Train set: accuracy = 83.798% - F1-score = 0.635897
Validation set:  accuracy = 72.788% - F1-score = 0.353241


=== Experiment 15/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.6944
Train set: accuracy = 84.268% - F1-score = 0.634633
Validation set:  accuracy = 73.739% - F1-score = 0.386749


=== Experiment 16/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.8421
Train set: accuracy = 84.486% - F1-score = 0.618351
Validation set:  accuracy = 74.215% - F1-score = 0.393721


=== Experiment 17/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.6944
Train set: accuracy = 80.927% - F1-score = 0.559669
Validation set:  accuracy = 76.213% - F1-score = 0.400838


=== Experiment 18/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.9087
Train set: accuracy = 82.639% - F1-score = 0.581851
Validation set:  accuracy = 73.073% - F1-score = 0.397567


=== Experiment 19/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.2079
Train set: accuracy = 83.630% - F1-score = 0.638372
Validation set:  accuracy = 75.737% - F1-score = 0.429979


=== Experiment 20/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.9225
Train set: accuracy = 84.352% - F1-score = 0.631232
Validation set:  accuracy = 72.978% - F1-score = 0.427732


=== Experiment 21/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.0898
Train set: accuracy = 87.945% - F1-score = 0.714217
Validation set:  accuracy = 73.739% - F1-score = 0.430618


=== Experiment 22/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.5287
Train set: accuracy = 87.693% - F1-score = 0.726556
Validation set:  accuracy = 74.500% - F1-score = 0.450289


=== Experiment 23/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.6561
Train set: accuracy = 88.936% - F1-score = 0.782987
Validation set:  accuracy = 72.598% - F1-score = 0.436517


=== Experiment 24/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.9007
Train set: accuracy = 87.223% - F1-score = 0.723352
Validation set:  accuracy = 72.598% - F1-score = 0.446629


=== Experiment 25/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.9970
Train set: accuracy = 87.089% - F1-score = 0.734638
Validation set:  accuracy = 73.930% - F1-score = 0.401057


=== Experiment 26/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 2.3009
Train set: accuracy = 86.568% - F1-score = 0.742817
Validation set:  accuracy = 75.452% - F1-score = 0.453766


=== Experiment 27/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.1547
Train set: accuracy = 89.825% - F1-score = 0.795342
Validation set:  accuracy = 72.598% - F1-score = 0.420970


=== Experiment 28/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.2521
Train set: accuracy = 90.379% - F1-score = 0.797592
Validation set:  accuracy = 74.310% - F1-score = 0.468290


=== Experiment 29/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.4163
Train set: accuracy = 87.962% - F1-score = 0.757164
Validation set:  accuracy = 75.357% - F1-score = 0.439344


=== Experiment 30/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.3523
Train set: accuracy = 87.911% - F1-score = 0.740387
Validation set:  accuracy = 74.120% - F1-score = 0.474652


=== Experiment 31/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 2.6129
Train set: accuracy = 83.445% - F1-score = 0.617598
Validation set:  accuracy = 74.691% - F1-score = 0.361832


=== Experiment 32/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.8800
Train set: accuracy = 83.193% - F1-score = 0.631326
Validation set:  accuracy = 71.836% - F1-score = 0.393333


=== Experiment 33/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.8969
Train set: accuracy = 84.486% - F1-score = 0.624590
Validation set:  accuracy = 73.644% - F1-score = 0.431918


=== Experiment 34/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.2710
Train set: accuracy = 83.647% - F1-score = 0.610553
Validation set:  accuracy = 74.310% - F1-score = 0.427882


=== Experiment 35/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.8147
Train set: accuracy = 82.018% - F1-score = 0.565996
Validation set:  accuracy = 75.547% - F1-score = 0.405306


=== Experiment 36/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.2253
Train set: accuracy = 82.455% - F1-score = 0.568428
Validation set:  accuracy = 70.885% - F1-score = 0.338999


=== Experiment 37/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.4267
Train set: accuracy = 82.354% - F1-score = 0.605977
Validation set:  accuracy = 74.310% - F1-score = 0.434412


=== Experiment 38/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.1649
Train set: accuracy = 81.582% - F1-score = 0.579896
Validation set:  accuracy = 73.834% - F1-score = 0.404270


=== Experiment 39/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.2257
Train set: accuracy = 85.460% - F1-score = 0.651299
Validation set:  accuracy = 73.549% - F1-score = 0.433555


=== Experiment 40/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.8648
Train set: accuracy = 85.040% - F1-score = 0.635350
Validation set:  accuracy = 74.310% - F1-score = 0.449044


=== Experiment 41/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 4.5315
Train set: accuracy = 85.074% - F1-score = 0.670766
Validation set:  accuracy = 73.168% - F1-score = 0.367018


=== Experiment 42/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.9233
Train set: accuracy = 84.050% - F1-score = 0.631852
Validation set:  accuracy = 72.312% - F1-score = 0.404596


=== Experiment 43/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.5836
Train set: accuracy = 83.445% - F1-score = 0.621771
Validation set:  accuracy = 72.788% - F1-score = 0.431685


=== Experiment 44/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 2.2096
Train set: accuracy = 82.908% - F1-score = 0.620411
Validation set:  accuracy = 74.500% - F1-score = 0.408256


=== Experiment 45/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.3607
Train set: accuracy = 84.940% - F1-score = 0.652686
Validation set:  accuracy = 73.644% - F1-score = 0.438453


=== Experiment 46/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.4375
Train set: accuracy = 85.494% - F1-score = 0.667333
Validation set:  accuracy = 74.500% - F1-score = 0.446666


=== Experiment 47/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.3795
Train set: accuracy = 83.546% - F1-score = 0.621646
Validation set:  accuracy = 74.215% - F1-score = 0.403242


=== Experiment 48/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.4739
Train set: accuracy = 82.505% - F1-score = 0.607666
Validation set:  accuracy = 74.120% - F1-score = 0.388582


=== Experiment 49/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.6725
Train set: accuracy = 79.214% - F1-score = 0.503736
Validation set:  accuracy = 73.073% - F1-score = 0.401056


=== Experiment 50/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.5159
Train set: accuracy = 78.694% - F1-score = 0.500900
Validation set:  accuracy = 72.502% - F1-score = 0.408339


=== Experiment 51/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.1999
Train set: accuracy = 79.768% - F1-score = 0.461236
Validation set:  accuracy = 68.887% - F1-score = 0.321892


=== Experiment 52/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.4279
Train set: accuracy = 79.718% - F1-score = 0.468050
Validation set:  accuracy = 72.693% - F1-score = 0.319146


=== Experiment 53/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.4564
Train set: accuracy = 78.274% - F1-score = 0.421066
Validation set:  accuracy = 70.790% - F1-score = 0.292957


=== Experiment 54/504 ===
Params: {'decay': 0.0001, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.6411
Train set: accuracy = 78.140% - F1-score = 0.458674
Validation set:  accuracy = 70.980% - F1-score = 0.299518


=== Experiment 55/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.7580
Train set: accuracy = 75.336% - F1-score = 0.435493
Validation set:  accuracy = 74.120% - F1-score = 0.363210


=== Experiment 56/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.5154
Train set: accuracy = 76.561% - F1-score = 0.454362
Validation set:  accuracy = 73.549% - F1-score = 0.393308


=== Experiment 57/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.5804
Train set: accuracy = 79.265% - F1-score = 0.535756
Validation set:  accuracy = 71.170% - F1-score = 0.372336


=== Experiment 58/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 3.0381
Train set: accuracy = 77.485% - F1-score = 0.489326
Validation set:  accuracy = 74.786% - F1-score = 0.392619


=== Experiment 59/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.0996
Train set: accuracy = 78.878% - F1-score = 0.527449
Validation set:  accuracy = 73.454% - F1-score = 0.407163


=== Experiment 60/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.0668
Train set: accuracy = 78.425% - F1-score = 0.530012
Validation set:  accuracy = 73.454% - F1-score = 0.374922


=== Experiment 61/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.6555
Train set: accuracy = 77.418% - F1-score = 0.498500
Validation set:  accuracy = 71.456% - F1-score = 0.374408


=== Experiment 62/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 2.7393
Train set: accuracy = 76.561% - F1-score = 0.476453
Validation set:  accuracy = 74.691% - F1-score = 0.422209


=== Experiment 63/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.4906
Train set: accuracy = 77.837% - F1-score = 0.519852
Validation set:  accuracy = 75.452% - F1-score = 0.401275


=== Experiment 64/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.1650
Train set: accuracy = 78.341% - F1-score = 0.532999
Validation set:  accuracy = 72.122% - F1-score = 0.387653


=== Experiment 65/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.7589
Train set: accuracy = 77.821% - F1-score = 0.510891
Validation set:  accuracy = 73.644% - F1-score = 0.402135


=== Experiment 66/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.7079
Train set: accuracy = 77.586% - F1-score = 0.506523
Validation set:  accuracy = 70.695% - F1-score = 0.391945


=== Experiment 67/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.3081
Train set: accuracy = 74.933% - F1-score = 0.388318
Validation set:  accuracy = 71.931% - F1-score = 0.318402


=== Experiment 68/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.1239
Train set: accuracy = 75.336% - F1-score = 0.395082
Validation set:  accuracy = 71.836% - F1-score = 0.322662


=== Experiment 69/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.3988
Train set: accuracy = 76.007% - F1-score = 0.423599
Validation set:  accuracy = 72.122% - F1-score = 0.344924


=== Experiment 70/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 3.4240
Train set: accuracy = 75.168% - F1-score = 0.385886
Validation set:  accuracy = 71.551% - F1-score = 0.331958


=== Experiment 71/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 3.8809
Train set: accuracy = 74.429% - F1-score = 0.366214
Validation set:  accuracy = 71.265% - F1-score = 0.313495


=== Experiment 72/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.5473
Train set: accuracy = 74.278% - F1-score = 0.364129
Validation set:  accuracy = 69.743% - F1-score = 0.292141


=== Experiment 73/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.5174
Train set: accuracy = 76.830% - F1-score = 0.434593
Validation set:  accuracy = 71.551% - F1-score = 0.333300


=== Experiment 74/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.7711
Train set: accuracy = 76.679% - F1-score = 0.444964
Validation set:  accuracy = 70.885% - F1-score = 0.308959


=== Experiment 75/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.7652
Train set: accuracy = 78.878% - F1-score = 0.526455
Validation set:  accuracy = 74.691% - F1-score = 0.409510


=== Experiment 76/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.1916
Train set: accuracy = 77.821% - F1-score = 0.465375
Validation set:  accuracy = 71.836% - F1-score = 0.348339


=== Experiment 77/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.2981
Train set: accuracy = 78.140% - F1-score = 0.470112
Validation set:  accuracy = 72.978% - F1-score = 0.419099


=== Experiment 78/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.4511
Train set: accuracy = 77.367% - F1-score = 0.479877
Validation set:  accuracy = 71.931% - F1-score = 0.380417


=== Experiment 79/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.6757
Train set: accuracy = 77.283% - F1-score = 0.479801
Validation set:  accuracy = 73.264% - F1-score = 0.364917


=== Experiment 80/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.4123
Train set: accuracy = 76.595% - F1-score = 0.472861
Validation set:  accuracy = 71.931% - F1-score = 0.340183


=== Experiment 81/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.4879
Train set: accuracy = 77.686% - F1-score = 0.501741
Validation set:  accuracy = 71.836% - F1-score = 0.369772


=== Experiment 82/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.0689
Train set: accuracy = 77.586% - F1-score = 0.512437
Validation set:  accuracy = 76.213% - F1-score = 0.400861


=== Experiment 83/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.6055
Train set: accuracy = 76.293% - F1-score = 0.444431
Validation set:  accuracy = 73.739% - F1-score = 0.340558


=== Experiment 84/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 2.0081
Train set: accuracy = 76.192% - F1-score = 0.442835
Validation set:  accuracy = 73.454% - F1-score = 0.405834


=== Experiment 85/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.8104
Train set: accuracy = 73.388% - F1-score = 0.339902
Validation set:  accuracy = 70.219% - F1-score = 0.305339


=== Experiment 86/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.6947
Train set: accuracy = 72.095% - F1-score = 0.309796
Validation set:  accuracy = 71.361% - F1-score = 0.310199


=== Experiment 87/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 3.3255
Train set: accuracy = 72.011% - F1-score = 0.316912
Validation set:  accuracy = 71.741% - F1-score = 0.303665


=== Experiment 88/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.3757
Train set: accuracy = 71.541% - F1-score = 0.327101
Validation set:  accuracy = 72.027% - F1-score = 0.295104


=== Experiment 89/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.1744
Train set: accuracy = 71.105% - F1-score = 0.301850
Validation set:  accuracy = 71.361% - F1-score = 0.299093


=== Experiment 90/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.1264
Train set: accuracy = 70.433% - F1-score = 0.328167
Validation set:  accuracy = 67.555% - F1-score = 0.294395


=== Experiment 91/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.4182
Train set: accuracy = 74.715% - F1-score = 0.356601
Validation set:  accuracy = 73.073% - F1-score = 0.295211


=== Experiment 92/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.4054
Train set: accuracy = 74.278% - F1-score = 0.348453
Validation set:  accuracy = 71.741% - F1-score = 0.314796


=== Experiment 93/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.4494
Train set: accuracy = 76.461% - F1-score = 0.437749
Validation set:  accuracy = 73.073% - F1-score = 0.329169


=== Experiment 94/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.2912
Train set: accuracy = 74.983% - F1-score = 0.378819
Validation set:  accuracy = 72.598% - F1-score = 0.296110


=== Experiment 95/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.8487
Train set: accuracy = 75.269% - F1-score = 0.360485
Validation set:  accuracy = 73.454% - F1-score = 0.323841


=== Experiment 96/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.5205
Train set: accuracy = 75.722% - F1-score = 0.372230
Validation set:  accuracy = 70.695% - F1-score = 0.285090


=== Experiment 97/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.6823
Train set: accuracy = 74.849% - F1-score = 0.359182
Validation set:  accuracy = 69.838% - F1-score = 0.263118


=== Experiment 98/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.3272
Train set: accuracy = 73.875% - F1-score = 0.321426
Validation set:  accuracy = 70.790% - F1-score = 0.284675


=== Experiment 99/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.1531
Train set: accuracy = 74.429% - F1-score = 0.338761
Validation set:  accuracy = 70.409% - F1-score = 0.268014


=== Experiment 100/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.6472
Train set: accuracy = 74.093% - F1-score = 0.321658
Validation set:  accuracy = 72.027% - F1-score = 0.284449


=== Experiment 101/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.3866
Train set: accuracy = 73.422% - F1-score = 0.311953
Validation set:  accuracy = 70.124% - F1-score = 0.271286


=== Experiment 102/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 2.2323
Train set: accuracy = 72.918% - F1-score = 0.279992
Validation set:  accuracy = 69.648% - F1-score = 0.238313


=== Experiment 103/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 3.0188
Train set: accuracy = 70.937% - F1-score = 0.274253
Validation set:  accuracy = 68.792% - F1-score = 0.237815


=== Experiment 104/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.3809
Train set: accuracy = 70.517% - F1-score = 0.249207
Validation set:  accuracy = 71.456% - F1-score = 0.233667


=== Experiment 105/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.8942
Train set: accuracy = 69.980% - F1-score = 0.241446
Validation set:  accuracy = 70.219% - F1-score = 0.236778


=== Experiment 106/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.2611
Train set: accuracy = 70.920% - F1-score = 0.262964
Validation set:  accuracy = 69.267% - F1-score = 0.266339


=== Experiment 107/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.3438
Train set: accuracy = 70.366% - F1-score = 0.269007
Validation set:  accuracy = 69.458% - F1-score = 0.269516


=== Experiment 108/504 ===
Params: {'decay': 0.0001, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.3937
Train set: accuracy = 70.316% - F1-score = 0.280686
Validation set:  accuracy = 64.795% - F1-score = 0.242545


=== Experiment 109/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.3305
Train set: accuracy = 84.066% - F1-score = 0.649212
Validation set:  accuracy = 72.217% - F1-score = 0.413213


=== Experiment 110/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.6268
Train set: accuracy = 84.318% - F1-score = 0.651185
Validation set:  accuracy = 72.883% - F1-score = 0.412188


=== Experiment 111/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.2244
Train set: accuracy = 87.928% - F1-score = 0.750285
Validation set:  accuracy = 72.693% - F1-score = 0.422652


=== Experiment 112/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.5549
Train set: accuracy = 87.811% - F1-score = 0.768848
Validation set:  accuracy = 73.359% - F1-score = 0.402742


=== Experiment 113/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.7533
Train set: accuracy = 87.794% - F1-score = 0.764404
Validation set:  accuracy = 74.691% - F1-score = 0.492948


=== Experiment 114/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.9386
Train set: accuracy = 85.645% - F1-score = 0.717449
Validation set:  accuracy = 74.215% - F1-score = 0.497632


=== Experiment 115/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.1610
Train set: accuracy = 85.846% - F1-score = 0.707998
Validation set:  accuracy = 72.502% - F1-score = 0.397037


=== Experiment 116/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.4850
Train set: accuracy = 87.895% - F1-score = 0.773357
Validation set:  accuracy = 73.359% - F1-score = 0.431780


=== Experiment 117/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.9531
Train set: accuracy = 88.398% - F1-score = 0.789191
Validation set:  accuracy = 74.691% - F1-score = 0.427446


=== Experiment 118/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.6659
Train set: accuracy = 87.475% - F1-score = 0.770521
Validation set:  accuracy = 71.170% - F1-score = 0.400387


=== Experiment 119/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.7501
Train set: accuracy = 86.803% - F1-score = 0.741643
Validation set:  accuracy = 72.693% - F1-score = 0.432323


=== Experiment 120/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.3701
Train set: accuracy = 86.702% - F1-score = 0.712902
Validation set:  accuracy = 72.978% - F1-score = 0.452149


=== Experiment 121/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.2788
Train set: accuracy = 83.445% - F1-score = 0.639148
Validation set:  accuracy = 72.788% - F1-score = 0.402591


=== Experiment 122/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.4722
Train set: accuracy = 84.234% - F1-score = 0.647504
Validation set:  accuracy = 71.456% - F1-score = 0.368230


=== Experiment 123/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.6370
Train set: accuracy = 84.419% - F1-score = 0.632563
Validation set:  accuracy = 72.788% - F1-score = 0.441874


=== Experiment 124/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.7232
Train set: accuracy = 84.520% - F1-score = 0.628121
Validation set:  accuracy = 71.931% - F1-score = 0.370588


=== Experiment 125/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.4553
Train set: accuracy = 82.656% - F1-score = 0.599778
Validation set:  accuracy = 73.454% - F1-score = 0.360766


=== Experiment 126/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.6463
Train set: accuracy = 82.236% - F1-score = 0.575504
Validation set:  accuracy = 74.976% - F1-score = 0.419151


=== Experiment 127/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.7725
Train set: accuracy = 82.421% - F1-score = 0.607950
Validation set:  accuracy = 76.308% - F1-score = 0.421825


=== Experiment 128/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 2.3980
Train set: accuracy = 82.522% - F1-score = 0.598335
Validation set:  accuracy = 72.978% - F1-score = 0.407228


=== Experiment 129/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.6758
Train set: accuracy = 87.055% - F1-score = 0.704384
Validation set:  accuracy = 72.978% - F1-score = 0.412995


=== Experiment 130/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.4786
Train set: accuracy = 87.324% - F1-score = 0.696972
Validation set:  accuracy = 72.693% - F1-score = 0.437076


=== Experiment 131/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.1985
Train set: accuracy = 87.710% - F1-score = 0.725866
Validation set:  accuracy = 73.739% - F1-score = 0.456657


=== Experiment 132/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.9360
Train set: accuracy = 88.633% - F1-score = 0.764296
Validation set:  accuracy = 74.120% - F1-score = 0.414244


=== Experiment 133/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 4.1767
Train set: accuracy = 87.391% - F1-score = 0.750419
Validation set:  accuracy = 75.833% - F1-score = 0.423411


=== Experiment 134/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.1414
Train set: accuracy = 87.576% - F1-score = 0.737374
Validation set:  accuracy = 71.836% - F1-score = 0.439130


=== Experiment 135/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.6749
Train set: accuracy = 90.128% - F1-score = 0.792375
Validation set:  accuracy = 73.073% - F1-score = 0.415154


=== Experiment 136/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.0559
Train set: accuracy = 89.825% - F1-score = 0.791182
Validation set:  accuracy = 73.549% - F1-score = 0.464977


=== Experiment 137/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.0009
Train set: accuracy = 88.264% - F1-score = 0.741740
Validation set:  accuracy = 74.691% - F1-score = 0.452566


=== Experiment 138/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.2874
Train set: accuracy = 87.559% - F1-score = 0.740420
Validation set:  accuracy = 73.644% - F1-score = 0.419713


=== Experiment 139/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.2752
Train set: accuracy = 82.203% - F1-score = 0.600057
Validation set:  accuracy = 74.500% - F1-score = 0.439533


=== Experiment 140/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.5138
Train set: accuracy = 82.673% - F1-score = 0.595886
Validation set:  accuracy = 72.788% - F1-score = 0.408169


=== Experiment 141/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.6692
Train set: accuracy = 83.479% - F1-score = 0.626841
Validation set:  accuracy = 73.930% - F1-score = 0.400944


=== Experiment 142/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.5611
Train set: accuracy = 84.520% - F1-score = 0.634873
Validation set:  accuracy = 73.454% - F1-score = 0.414230


=== Experiment 143/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 3.4460
Train set: accuracy = 82.807% - F1-score = 0.566628
Validation set:  accuracy = 74.405% - F1-score = 0.380301


=== Experiment 144/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.7781
Train set: accuracy = 82.723% - F1-score = 0.562478
Validation set:  accuracy = 72.788% - F1-score = 0.353237


=== Experiment 145/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 2.6579
Train set: accuracy = 81.817% - F1-score = 0.599716
Validation set:  accuracy = 73.168% - F1-score = 0.388732


=== Experiment 146/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.3426
Train set: accuracy = 82.707% - F1-score = 0.599238
Validation set:  accuracy = 74.310% - F1-score = 0.421967


=== Experiment 147/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.2997
Train set: accuracy = 84.637% - F1-score = 0.643247
Validation set:  accuracy = 74.215% - F1-score = 0.440463


=== Experiment 148/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.5437
Train set: accuracy = 85.124% - F1-score = 0.643768
Validation set:  accuracy = 73.739% - F1-score = 0.448544


=== Experiment 149/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.6223
Train set: accuracy = 84.167% - F1-score = 0.615538
Validation set:  accuracy = 72.693% - F1-score = 0.413251


=== Experiment 150/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.7726
Train set: accuracy = 85.460% - F1-score = 0.659584
Validation set:  accuracy = 71.170% - F1-score = 0.402569


=== Experiment 151/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.2647
Train set: accuracy = 83.865% - F1-score = 0.639986
Validation set:  accuracy = 75.167% - F1-score = 0.403644


=== Experiment 152/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.9216
Train set: accuracy = 83.831% - F1-score = 0.640294
Validation set:  accuracy = 71.075% - F1-score = 0.423160


=== Experiment 153/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.0913
Train set: accuracy = 85.057% - F1-score = 0.647362
Validation set:  accuracy = 74.500% - F1-score = 0.452413


=== Experiment 154/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.3015
Train set: accuracy = 84.184% - F1-score = 0.627055
Validation set:  accuracy = 73.549% - F1-score = 0.434800


=== Experiment 155/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.4260
Train set: accuracy = 83.596% - F1-score = 0.627217
Validation set:  accuracy = 73.930% - F1-score = 0.426116


=== Experiment 156/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.1090
Train set: accuracy = 84.839% - F1-score = 0.637809
Validation set:  accuracy = 70.790% - F1-score = 0.380911


=== Experiment 157/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 2.0207
Train set: accuracy = 78.677% - F1-score = 0.490518
Validation set:  accuracy = 72.122% - F1-score = 0.342898


=== Experiment 158/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.6609
Train set: accuracy = 78.475% - F1-score = 0.479572
Validation set:  accuracy = 71.361% - F1-score = 0.342268


=== Experiment 159/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.3079
Train set: accuracy = 79.819% - F1-score = 0.517336
Validation set:  accuracy = 70.980% - F1-score = 0.371551


=== Experiment 160/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 3.0972
Train set: accuracy = 80.188% - F1-score = 0.511958
Validation set:  accuracy = 73.739% - F1-score = 0.407533


=== Experiment 161/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.6275
Train set: accuracy = 79.113% - F1-score = 0.481530
Validation set:  accuracy = 72.598% - F1-score = 0.348883


=== Experiment 162/504 ===
Params: {'decay': 1e-05, 'dropout': 0.2, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.0980
Train set: accuracy = 77.972% - F1-score = 0.452126
Validation set:  accuracy = 72.788% - F1-score = 0.351440


=== Experiment 163/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.0107
Train set: accuracy = 76.847% - F1-score = 0.475686
Validation set:  accuracy = 72.598% - F1-score = 0.377517


=== Experiment 164/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.5056
Train set: accuracy = 76.797% - F1-score = 0.467024
Validation set:  accuracy = 72.502% - F1-score = 0.364508


=== Experiment 165/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.0663
Train set: accuracy = 78.240% - F1-score = 0.503610
Validation set:  accuracy = 72.788% - F1-score = 0.381139


=== Experiment 166/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.6625
Train set: accuracy = 77.972% - F1-score = 0.491451
Validation set:  accuracy = 71.170% - F1-score = 0.386524


=== Experiment 167/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.3238
Train set: accuracy = 77.955% - F1-score = 0.509252
Validation set:  accuracy = 73.834% - F1-score = 0.362465


=== Experiment 168/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.4412
Train set: accuracy = 77.754% - F1-score = 0.506322
Validation set:  accuracy = 74.405% - F1-score = 0.408769


=== Experiment 169/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.3874
Train set: accuracy = 77.921% - F1-score = 0.523487
Validation set:  accuracy = 73.739% - F1-score = 0.424101


=== Experiment 170/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.2151
Train set: accuracy = 77.569% - F1-score = 0.515228
Validation set:  accuracy = 74.786% - F1-score = 0.421351


=== Experiment 171/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.5200
Train set: accuracy = 78.761% - F1-score = 0.534044
Validation set:  accuracy = 72.502% - F1-score = 0.410397


=== Experiment 172/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 2.3717
Train set: accuracy = 77.921% - F1-score = 0.510198
Validation set:  accuracy = 73.454% - F1-score = 0.374984


=== Experiment 173/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.2198
Train set: accuracy = 77.468% - F1-score = 0.485392
Validation set:  accuracy = 69.648% - F1-score = 0.318868


=== Experiment 174/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.4685
Train set: accuracy = 77.871% - F1-score = 0.514109
Validation set:  accuracy = 72.693% - F1-score = 0.377791


=== Experiment 175/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 2.1416
Train set: accuracy = 75.269% - F1-score = 0.383550
Validation set:  accuracy = 67.364% - F1-score = 0.269495


=== Experiment 176/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.6311
Train set: accuracy = 75.084% - F1-score = 0.384308
Validation set:  accuracy = 70.029% - F1-score = 0.333086


=== Experiment 177/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.5247
Train set: accuracy = 75.655% - F1-score = 0.388065
Validation set:  accuracy = 71.265% - F1-score = 0.312876


=== Experiment 178/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.8964
Train set: accuracy = 75.705% - F1-score = 0.409163
Validation set:  accuracy = 72.693% - F1-score = 0.329209


=== Experiment 179/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.0836
Train set: accuracy = 74.060% - F1-score = 0.363531
Validation set:  accuracy = 72.122% - F1-score = 0.362212


=== Experiment 180/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.7090
Train set: accuracy = 74.043% - F1-score = 0.332125
Validation set:  accuracy = 71.075% - F1-score = 0.325547


=== Experiment 181/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.4219
Train set: accuracy = 75.739% - F1-score = 0.399950
Validation set:  accuracy = 73.454% - F1-score = 0.361132


=== Experiment 182/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.4206
Train set: accuracy = 76.780% - F1-score = 0.434777
Validation set:  accuracy = 71.265% - F1-score = 0.336284


=== Experiment 183/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.7222
Train set: accuracy = 77.569% - F1-score = 0.479496
Validation set:  accuracy = 74.596% - F1-score = 0.396371


=== Experiment 184/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.5268
Train set: accuracy = 78.156% - F1-score = 0.460146
Validation set:  accuracy = 72.217% - F1-score = 0.341351


=== Experiment 185/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.5982
Train set: accuracy = 78.022% - F1-score = 0.490920
Validation set:  accuracy = 74.310% - F1-score = 0.380977


=== Experiment 186/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.0930
Train set: accuracy = 76.494% - F1-score = 0.457594
Validation set:  accuracy = 73.454% - F1-score = 0.344487


=== Experiment 187/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.2783
Train set: accuracy = 76.360% - F1-score = 0.471354
Validation set:  accuracy = 73.454% - F1-score = 0.356428


=== Experiment 188/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.5615
Train set: accuracy = 78.190% - F1-score = 0.501569
Validation set:  accuracy = 70.599% - F1-score = 0.345030


=== Experiment 189/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.3467
Train set: accuracy = 78.341% - F1-score = 0.499682
Validation set:  accuracy = 72.883% - F1-score = 0.428744


=== Experiment 190/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.8582
Train set: accuracy = 77.283% - F1-score = 0.495962
Validation set:  accuracy = 72.693% - F1-score = 0.393972


=== Experiment 191/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.4185
Train set: accuracy = 76.192% - F1-score = 0.400863
Validation set:  accuracy = 69.933% - F1-score = 0.321356


=== Experiment 192/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.5880
Train set: accuracy = 76.645% - F1-score = 0.471872
Validation set:  accuracy = 73.264% - F1-score = 0.388260


=== Experiment 193/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.8310
Train set: accuracy = 71.827% - F1-score = 0.308581
Validation set:  accuracy = 70.885% - F1-score = 0.290202


=== Experiment 194/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.7353
Train set: accuracy = 72.347% - F1-score = 0.309497
Validation set:  accuracy = 70.409% - F1-score = 0.289991


=== Experiment 195/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 1.2855
Train set: accuracy = 71.944% - F1-score = 0.285846
Validation set:  accuracy = 68.696% - F1-score = 0.248201


=== Experiment 196/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.7188
Train set: accuracy = 70.248% - F1-score = 0.302588
Validation set:  accuracy = 70.314% - F1-score = 0.284215


=== Experiment 197/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.3409
Train set: accuracy = 71.323% - F1-score = 0.308790
Validation set:  accuracy = 67.840% - F1-score = 0.277556


=== Experiment 198/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [512, 256, 128], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.5255
Train set: accuracy = 71.021% - F1-score = 0.296958
Validation set:  accuracy = 70.314% - F1-score = 0.304106


=== Experiment 199/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.2688
Train set: accuracy = 74.429% - F1-score = 0.348857
Validation set:  accuracy = 72.407% - F1-score = 0.312390


=== Experiment 200/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.9059
Train set: accuracy = 75.319% - F1-score = 0.398681
Validation set:  accuracy = 73.454% - F1-score = 0.310195


=== Experiment 201/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.2352
Train set: accuracy = 75.991% - F1-score = 0.404021
Validation set:  accuracy = 73.359% - F1-score = 0.318104


=== Experiment 202/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.3779
Train set: accuracy = 76.377% - F1-score = 0.420034
Validation set:  accuracy = 73.834% - F1-score = 0.309983


=== Experiment 203/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.6269
Train set: accuracy = 76.158% - F1-score = 0.376070
Validation set:  accuracy = 73.834% - F1-score = 0.319374


=== Experiment 204/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.01, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 0.5321
Train set: accuracy = 76.175% - F1-score = 0.387982
Validation set:  accuracy = 73.264% - F1-score = 0.366105


=== Experiment 205/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.6269
Train set: accuracy = 73.858% - F1-score = 0.293343
Validation set:  accuracy = 70.409% - F1-score = 0.253815


=== Experiment 206/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.5830
Train set: accuracy = 73.758% - F1-score = 0.359625
Validation set:  accuracy = 75.262% - F1-score = 0.327128


=== Experiment 207/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 2.7056
Train set: accuracy = 74.480% - F1-score = 0.332431
Validation set:  accuracy = 71.741% - F1-score = 0.301780


=== Experiment 208/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 4.1462
Train set: accuracy = 74.412% - F1-score = 0.323753
Validation set:  accuracy = 72.027% - F1-score = 0.274940


=== Experiment 209/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.1062
Train set: accuracy = 73.439% - F1-score = 0.322718
Validation set:  accuracy = 71.170% - F1-score = 0.280966


=== Experiment 210/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 3.4353
Train set: accuracy = 73.237% - F1-score = 0.291422
Validation set:  accuracy = 69.933% - F1-score = 0.240844


=== Experiment 211/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 1.1094
Train set: accuracy = 71.340% - F1-score = 0.261021
Validation set:  accuracy = 70.029% - F1-score = 0.265597


=== Experiment 212/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 16, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 373/373 - Loss: 0.6994
Train set: accuracy = 70.870% - F1-score = 0.244137
Validation set:  accuracy = 68.030% - F1-score = 0.214056


=== Experiment 213/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 2.3876
Train set: accuracy = 70.181% - F1-score = 0.281689
Validation set:  accuracy = 71.741% - F1-score = 0.312938


=== Experiment 214/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 32, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
[Epoch 30/30] Batch 187/187 - Loss: 0.5205
Train set: accuracy = 70.165% - F1-score = 0.261762
Validation set:  accuracy = 72.027% - F1-score = 0.268913


=== Experiment 215/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adam', 'test': False}
[Epoch 30/30] Batch 94/94 - Loss: 1.3376
Train set: accuracy = 70.248% - F1-score = 0.271220
Validation set:  accuracy = 70.219% - F1-score = 0.268092


=== Experiment 216/504 ===
Params: {'decay': 1e-05, 'dropout': 0.5, 'hidden_layers': [256, 128, 64], 'lr': 0.0001, 'max_iters': 30, 'nn_batch_size': 64, 'nn_type': 'mlp', 'optim': 'adamw', 'test': False}
usage: colab_kernel_launcher.py [-h] [--data DATA] [--nn_type NN_TYPE]
                                [--nn_batch_size NN_BATCH_SIZE]
                                [--device DEVICE]
                                [--hidden_layers HIDDEN_LAYERS [HIDDEN_LAYERS ...]]
                                [--lr LR] [--optim OPTIM] [--dropout DROPOUT]
                                [--decay DECAY] [--kernel KERNEL]
                                [--padding PADDING] [--max_iters MAX_ITERS]
                                [--test]
[Epoch 30/30] Batch 94/94 - Loss: 1.0966
Train set: accuracy = 70.920% - F1-score = 0.290753
Validation set:  accuracy = 69.933% - F1-score = 0.277361